{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HumanOrHorsePredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dwaGithub/machinelearning/blob/master/HumanOrHorsePredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RKt9V0zbxfKv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r Dataset-horse-or-human/  # first remove folder, if present\n",
        "!git clone https://github.com/dwaGithub/Dataset-horse-or-human  # replace with your Github acc."
      ],
      "metadata": {
        "id": "EGdQFCSFx9AJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17038eab-4a2d-412c-c469-282526090ddd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'machine-learning/': No such file or directory\n",
            "Cloning into 'Dataset-horse-or-human'...\n",
            "remote: Enumerating objects: 1292, done.\u001b[K\n",
            "remote: Total 1292 (delta 0), reused 0 (delta 0), pack-reused 1292\u001b[K\n",
            "Receiving objects: 100% (1292/1292), 153.49 MiB | 13.88 MiB/s, done.\n",
            "Checking out files: 100% (2566/2566), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "                      rescale=1./255,\n",
        "                      rotation_range=40,\n",
        "                      width_shift_range=0.2,\n",
        "                      height_shift_range=0.2,\n",
        "                      shear_range=0.2,\n",
        "                      zoom_range=0.2,\n",
        "                      horizontal_flip=True,\n",
        "                      fill_mode='nearest')\n",
        "trainingFiles = 'Dataset-horse-or-human/horse-or-human/train' # change according to your setup\n",
        "testFile = 'Dataset-horse-or-human/horse-or-human/validation'"
      ],
      "metadata": {
        "id": "phU5n4MVyKlO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read data from dataset\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    trainingFiles,\n",
        "    target_size= (300, 300),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "id": "wEnMA--7zrMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7381017a-d592-45ca-dd54-aa50fe9c5144"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the test set\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Reading the test set\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    testFile,\n",
        "    target_size= (300, 300),\n",
        "    batch_size=32,\n",
        "    class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CRQKLbh58Va",
        "outputId": "2d9ea256-23aa-4acd-ac86-19925dc1088b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 256 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv2D(16, (3,3), activation='relu',\n",
        "                             input_shape=(300, 300, 3)),\n",
        "      tf.keras.layers.MaxPooling2D(2, 2),\n",
        "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "Jaghgqqy6IPI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Voy8DMmg6dE7",
        "outputId": "76c8d86e-23bd-422c-ab44-ecf37773b57e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 298, 298, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 149, 149, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 147, 147, 32)      4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 73, 73, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 71, 71, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 35, 35, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 33, 33, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 16, 16, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 7, 7, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3136)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               1606144   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,704,097\n",
            "Trainable params: 1,704,097\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "       optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "       metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kEvVqsDr6f9d"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=15,\n",
        "    validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUBWCOYD6hjr",
        "outputId": "68f0a79e-b4be-4d52-d5fd-0a183370e405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "33/33 [==============================] - 113s 3s/step - loss: 0.7108 - accuracy: 0.6212 - val_loss: 0.5262 - val_accuracy: 0.7539\n",
            "Epoch 2/15\n",
            "33/33 [==============================] - 111s 3s/step - loss: 0.5912 - accuracy: 0.7089 - val_loss: 1.7154 - val_accuracy: 0.5117\n",
            "Epoch 3/15\n",
            "33/33 [==============================] - 113s 3s/step - loss: 0.4935 - accuracy: 0.7614 - val_loss: 0.6881 - val_accuracy: 0.6914\n",
            "Epoch 4/15\n",
            "10/33 [========>.....................] - ETA: 1:17 - loss: 0.5300 - accuracy: 0.7869"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retraining with epochs = 8\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=8,\n",
        "    validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "aIUaqTt6BS_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mymodel.h5')"
      ],
      "metadata": {
        "id": "4x_UA4vS-rXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path = 'machine-learning/horse-or-human/horse-or-human/validation/human/valhuman01-01.png'"
      ],
      "metadata": {
        "id": "tYTJ2BdW-toY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.preprocessing import image\n",
        "#import numpy as np\n",
        "\n",
        "# loads the image and resizes\n",
        "#img = image.load_img(path, target_size=(300, 300))\n",
        "#x = image.img_to_array(img)\n",
        "#x = np.expand_dims(x, axis=0)"
      ],
      "metadata": {
        "id": "s2al3md0-x0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classes = model.predict(x)\n",
        "#if classes[0]>0.5:\n",
        "#    print(\" Image is a human\")\n",
        "#else:\n",
        "#    print(\" Image is a horse\")"
      ],
      "metadata": {
        "id": "fB8JuQi_-0dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#img"
      ],
      "metadata": {
        "id": "hr2ihOWtAO3d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}